<p>A list of interesting papers/blogs/reports to read:

<h3>Causal Inference</h3>

<ol>

<li> On the Role of the Propensity Score in Efficient Semiparametric Estimation of Average Treatment Effects

<a href="https://www.jstor.org/stable/pdf/2998560.pdf">[Link]</a>

<p>What I Like: The proofs in the appendix are very well done.


<li>Nonparametric estimation of average treatment effects under exogeneity: A review 

<a href="http://www.stat.columbia.edu/~gelman/stuff_for_blog/imbens.pdf">[Link]</a> 

<p>What I Like: A great survey of Average Treatment Effect Estimation using propensity score models. One of the few that attempts to discuss the variance of the estimators.

<li>The intuition behind inverse probability weighting in causal inference. 

<a href="http://www.rebeccabarter.com/blog/2017-07-05-ip-weighting/">[Link]</a>

<p>What I Like: The proof of the IPTW estimator being unbiased estimate of the Avergae Treatment effect is super easy to follow. 

<li> Applied Statistics Lecture Notes, Princeton

<a href="https://imai.fas.harvard.edu/teaching/files/intro-stat.pdf">[Link]</a>

<p>What I Like: Intro to metrics in Causal Inference Literature, connections to hypothesis testing. 


</ol>

<h3> Active Learning</h3>

<ol>

<li>Active Learning for Logistic Regression: An Evaluation

<a href="https://repository.upenn.edu/cgi/viewcontent.cgi?article=1378&context=cis_papers">[Link]</a>

<p>What I Like: The Decomposition of the Bias and Variance for a Classifier in L2 loss along with the Taylor Expansion of the Likelihood in the model parameters for A-Optimality. 

<li>A Variance Minimization Criterion to Active Learning on Graphs 

<a href="http://proceedings.mlr.press/v22/ji12/ji12.pdf">[Link]</a>

<p>What I Like: Connection of Gaussian Random Fields to Graph Laplacian and Covariance. Optimizing the Trace of this Covariance.

</ol>

<h3>Statistcal Theory/Information Theory</h3>
<ol>
<li> Stanford Stats 311 Lecture Notes

<a href="https://web.stanford.edu/class/stats311/Lectures/full_notes.pdf">[Link]</a>

<p>What I Like: Exponential Families and Fisher Information, general description of f-Divergences.

<li> Domain Adaptation: Learning Bounds and Algorithms

<a href="https://arxiv.org/abs/0902.3430">[Link]</a>
</ol>
<h3>Approximate/Exact Inference</h3>
<ol>
<li> Understanding the Variational Lower Bound </li>

<a href="http://legacydirs.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf">[Link]</a>

<p>What I Like: The final example about Object Recogntion, makes VI much more intuitive and less of a dark art as it appears in other explanations. 

